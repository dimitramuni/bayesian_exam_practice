---
title: "practice"
author: "Dimitra Muni"
date: "8/16/2020"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```


# 2020-06-04

## Help Code
```{r}
# Bayesian Learning Exam 2020-06-04
# Run this file once during the exam to get all the required data and functions for the exam in working memory 
# Author: Per Siden

###############################
########## Problem 1 ########## 
############################### 

###############################
########## Problem 2 ########## 
############################### 

###############################
########## Problem 3 ########## 
############################### 

# Reading the data from file
#load(file = 'titanic.RData')

if("mvtnorm" %in% rownames(installed.packages()) == FALSE) {install.packages("mvtnorm")}
if("msm" %in% rownames(installed.packages()) == FALSE) {install.packages("msm")}
library(mvtnorm) # For mulitvariate normal
library(msm) # For truncated normal

BayesProbReg <- function(y, X, mu_0, tau, nIter){
  # Gibbs sampling in probit regression using data augmentation:
  #
  # beta | tau ~ N(mu_0, tau^2*I)
  #
  # INPUTS:
  #   y - n-by-1 vector with response data observations
  #   X - n-by-nCovs matrix with covariates, first column should be ones if you want an intercept.
  #   mu_0 - prior mean for beta
  #   tau - prior standard deviation for beta
  #   nIter - Number of samples from the posterior (iterations)
  #
  # OUTPUTS:
  #   betaSample    - Posterior samples of beta.          nIter-by-nCovs matrix
  
  # Prior
  nPara <- dim(X)[2] # this line was missing in the exam
  priorCov <- tau^2*diag(nPara)
  priorPrec <- solve(priorCov)
  
  # Compute posterior hyperparameters
  n = length(y) # Number of observations
  n1 = sum(y)
  n0 = n - n1
  nCovs = dim(X)[2] # Number of covariates
  XX = t(X)%*%X
  
  # The actual sampling
  betaSample = matrix(NA, nIter, nCovs)
  u <- matrix(NA, n, 1)
  beta <- solve(XX,crossprod(X,y)) # OLS estimate as initial value
  for (i in 1:nIter){
    
    xBeta <- X%*%beta
    
    # Draw u | beta
    u[y == 0] <- rtnorm(n = n0, mean = xBeta[y==0], sd = 1, lower = -Inf, upper = 0)
    u[y == 1] <- rtnorm(n = n1, mean = xBeta[y==1], sd = 1, lower = 0, upper = Inf)
    
    # Draw beta | u
    betaHat <- solve(XX,t(X)%*%u)
    postPrec <- XX + priorPrec
    postCov <- solve(postPrec)
    betaMean <- solve(postPrec,XX%*%betaHat + priorPrec%*%mu_0)
    beta <- t(rmvnorm(n = 1, mean = betaMean, sigma = postCov))
    betaSample[i,] <- t(beta)
    
  }
  
  return(betaSample=betaSample)
}


###############################
########## Problem 4 ########## 
###############################



```


## Fish Classification

```{r}
f=dnorm(10,14,2*sqrt(17/16))*dnorm(250,mean=300,sd=50*sqrt(17/16))*.75
m=dnorm(10,12,2*sqrt(5/4))*dnorm(250,mean=280,sd=50*sqrt(5/4))*.25
f/(f+m)
```


## Binomial distribution

```{r}
#a

theta=seq(0,1,0.001)
unnormalized<- dbinom(33,50,theta)*(theta>0.3 & theta<0.7)
plot(dbeta(seq(0,1,0.001),34,51-33),type='l',col=1)
lines(unnormalized/(0.001*sum(unnormalized)),type='l',col=2)
legend('topleft',c('using U(0,1) prior','using U(0.3,0.7) prior'),col=c(1,2),lty=2)

x=33
gridstep <- 0.001
thetaGrid <- seq(0,1,gridstep)
unnormpost <- dbinom(x,50,thetaGrid)*(thetaGrid>0.3)*(thetaGrid<0.7)
postB <- (1/gridstep)*(unnormpost/sum(unnormpost))
plot(thetaGrid,postB,type="l",main="Posteriors",xlab="theta",ylab="")
lines(thetaGrid,dbeta(thetaGrid,x+1,51-x),type="l",col=2)
legend(x = 0, y = 8, legend = c("Prior B","Prior A"), 
       col = c("black","red"), lty = c(1,1), lwd = c(2,2), cex = 0.8)

#b


#theta=seq(0,1,0.001)
#unnormalized<- dbinom(33,50,theta)*(theta>0.3 & theta<0.7)
#priorBprob=sum(unnormalized/(0.001*sum(unnormalized)) <0.5)/length(theta)
#priorAprob=sum(dbeta(theta,34,18)<0.5)


x=33



ProbA <- pbeta(0.5,x+1,51-x)
ProbB=sum(postB[thetaGrid<=0.5]*0.001)
```



# 2018-11-01

## Helpfile
```{r}
# Bayesian Learning Exam 2018-11-01
# Run this file once during the exam to get all the required data and functions for the exam in working memory 
# Author: Per Siden

###############################
########## Problem 1 ########## 
############################### 


###############################
########## Problem 2 ########## 
############################### 

# Reading the data from file
#load(file = 'fish.RData')

install.packages("mvtnorm")
library(mvtnorm)

# Defining a function that simulates from the scaled inverse Chi-square distribution
rScaledInvChi2 <- function(n, df, scale){
  return((df*scale)/rchisq(n,df=df))
}

BayesLinReg <- function(y, X, mu_0, Omega_0, v_0, sigma2_0, nIter){
  # Direct sampling from a Gaussian linear regression with conjugate prior:
  #
  # beta | sigma2 ~ N(mu_0, sigma2*inv(Omega_0))
  # sigma2 ~ Inv-Chi2(v_0,sigma2_0)
  # 
  # Author: Mattias Villani, IDA, Linkoping University. http://mattiasvillani.com
  #
  # INPUTS:
  #   y - n-by-1 vector with response data observations
  #   X - n-by-nCovs matrix with covariates, first column should be ones if you want an intercept.
  #   mu_0 - prior mean for beta
  #   Omega_0  - prior precision matrix for beta
  #   v_0      - degrees of freedom in the prior for sigma2
  #   sigma2_0 - location ("best guess") in the prior for sigma2
  #   nIter - Number of samples from the posterior (iterations)
  #
  # OUTPUTS:
  #   results$betaSample     - Posterior sample of beta.     nIter-by-nCovs matrix
  #   results$sigma2Sample   - Posterior sample of sigma2.   nIter-by-1 vector
  
  # Compute posterior hyperparameters
  n = length(y) # Number of observations
  nCovs = dim(X)[2] # Number of covariates
  XX = t(X)%*%X
  betaHat <- solve(XX,t(X)%*%y)
  Omega_n = XX + Omega_0
  mu_n = solve(Omega_n,XX%*%betaHat+Omega_0%*%mu_0)
  v_n = v_0 + n
  sigma2_n = as.numeric((v_0*sigma2_0 + ( t(y)%*%y + t(mu_0)%*%Omega_0%*%mu_0 - t(mu_n)%*%Omega_n%*%mu_n))/v_n)
  invOmega_n = solve(Omega_n)
  
  # The actual sampling
  sigma2Sample = rep(NA, nIter)
  betaSample = matrix(NA, nIter, nCovs)
  for (i in 1:nIter){
    
    # Simulate from p(sigma2 | y, X)
    sigma2 = rScaledInvChi2(n=1, df = v_n, scale = sigma2_n)
    sigma2Sample[i] = sigma2
    
    # Simulate from p(beta | sigma2, y, X)
    beta_ = rmvnorm(n=1, mean = mu_n, sigma = sigma2*invOmega_n)
    betaSample[i,] = beta_
    
  }
  return(results = list(sigma2Sample = sigma2Sample, betaSample=betaSample))
}

###############################
########## Problem 3 ########## 
############################### 

# No code or data for this problem

###############################
########## Problem 4 ########## 
############################### 

# Reading the data from file
#load(file = 'weibull.RData')
```


## Prediction and Decision
```{r}

# a,b

y=c(1690, 1790,1760,1750)
ybar=mean(y)
x=rnorm(1000,mean = ybar,sd=50*sqrt(1+(1/4)))
hist(x,50)
m=c()
for (i in 1:1000) {
  
 w= rnorm(52,mean = ybar,sd=50*sqrt(1+(1/4)) ) 
 
 m=c(m,sum(w>1850))
 
 
}

hist(m)
mean(m)


#c



ExpectedLoss<-function(a, weekWeightYear){
  EL = a + mean(colSums(weekWeightYear>1000*log(a)))
  return(EL)
}

aGrid = seq(2,10,by = 0.01)
EL = rep(NA,length(aGrid),1)
count = 0
for (a in aGrid){
  count = count + 1
  EL[count] = ExpectedLoss(a, weekWeightYear)
}
plot(aGrid, EL, type = "l")
aOpt = aGrid[which.min(EL)] # This is the optimal a
points(x= aOpt, y = ExpectedLoss(a=aOpt, weekWeightYear), col = "red")
aOpt

```


## Regression

```{r}

```

